### A Guide to Integration Testing for the Multi-Modal Platform

Integration testing focuses on verifying the interactions and data flows between different software modules. Unlike unit
tests, which scrutinize components in isolation, integration tests are designed to expose faults in the interfaces and
interactions between integrated components. For this project, the primary goal is to validate the end-to-end
functionality of the distributed Hub-and-Spoke architecture.

#### **1. Scope and Objectives**

The integration testing for this platform must cover the critical communication pathways and data handoffs between the
PC Controller (Hub) and the Android Sensor Nodes (Spokes). The key objectives are to:

* Validate the entire **command and control loop**, from a user action on the PC GUI to the corresponding execution on
  the Android device and back.
* Verify the **temporal synchronization mechanism** across the distributed system to ensure it meets the stringent <5 ms
  accuracy requirement (NFR2).[1]
* Confirm the integrity of the **end-to-end data pipeline**, from sensor capture on the Android device to final
  aggregation on the PC.
* Test the system's **fault tolerance** and its ability to recover from common failures like network interruptions (
  FR8).[1]

#### **2. Methodology and Test Environment**

Integration tests for this system are primarily conducted as end-to-end system tests that involve running the actual,
compiled applications on physical hardware. While some interactions can be simulated for automated checks, the final
validation requires a real-world test environment.

* **Test Environment Setup:**
    * A host PC running the `pc_controller` application.
    * At least two Android smartphones running the `android_sensor_node` application.
    * A shared, stable Wi-Fi network to which all devices are connected.
    * The required hardware sensors: a Shimmer3 GSR+ sensor and a Topdon TC001 thermal camera.
* **Tools and Frameworks:**
    * The tests are executed by a human operator following a detailed test script.
    * Verification is performed by inspecting the application logs, the state of the PC GUI, and, most importantly, the
      final data files generated by the system.
    * For automated analysis of synchronization, a Python script using `pandas` and `OpenCV` can be used to process the
      output files and calculate timing discrepancies.

#### **3. Key Integration Test Scenarios**

The following test cases are designed to validate the most critical interactions within the system, directly mapping to
the functional requirements outlined in the project documentation.[1]

**Test Case 1: Device Discovery, Connection, and Capabilities Exchange**

* **Objective:** To verify that the PC Hub can successfully discover, connect to, and query the capabilities of an
  Android Spoke.
* **Procedure:**
    1. Launch the `pc_controller` application on the PC.
    2. Launch the `android_sensor_node` application on an Android device connected to the same Wi-Fi network.
    3. Observe the PC GUI's device list.
    4. Initiate a connection from the PC to the Android device.
* **Expected Outcome:**
    * The Android device appears in the PC's list of discoverable devices.
    * A stable TCP/IP connection is established.
    * The PC GUI updates the device's status to "Connected" and correctly displays its hardware capabilities (e.g.,
      available cameras) based on the initial JSON handshake.[1]

**Test Case 2: Synchronized Multi-Device Recording (FR2)**

* **Objective:** To verify that a single command from the Hub can synchronously start and stop a recording session
  across multiple devices.
* **Procedure:**
    1. Connect at least two Android Spokes to the PC Hub.
    2. On the PC GUI, click the "Start Session" button.
    3. Allow the recording to run for approximately 30 seconds.
    4. Click the "Stop Session" button.
* **Expected Outcome:**
    * All connected devices begin recording data simultaneously upon the "Start" command.
    * Data files (videos, CSVs) are created on each Android device.
    * All devices stop recording upon the "Stop" command.
    * The duration of the recorded files on each device should be approximately 30 seconds.

**Test Case 3: Temporal Synchronization Verification ("Flash Sync" Test) (FR3, FR7, NFR2)**

* **Objective:** To provide definitive, empirical validation that the system's time synchronization mechanism aligns
  data streams to within the required <5 ms tolerance.
* **Procedure:**
    1. Start a synchronized recording session with at least two Android Spokes.
    2. During the recording, click the "Flash Sync" button on the PC GUI at least three times at random intervals.
    3. Stop the session and allow the data to transfer to the PC.
    4. Analyze the resulting data files.
* **Expected Outcome:**
    * The recorded videos from each Android device must show a brief, bright white flash corresponding to each "Flash
      Sync" command.
    * A log file (`flash_sync_events.csv`) on each device must contain the high-precision timestamps of when each flash
      was triggered.
    * When the timestamps from all devices for a single flash event are adjusted using the NTP-calculated clock offsets,
      the final, aligned timestamps must differ by less than 5 milliseconds.[1] This confirms the system meets its most
      critical non-functional requirement.

**Test Case 4: End-to-End Data Pipeline Integrity (FR10)**

* **Objective:** To verify the complete data pipeline, from local recording on the Spoke to final aggregation on the
  Hub.
* **Procedure:**
    1. Run a complete, synchronized recording session (start, record for a short duration, stop).
    2. Monitor the PC GUI and the file system on the PC.
* **Expected Outcome:**
    * After the session stops, the Android app must automatically compress its session data into a ZIP archive.
    * This archive must be successfully transferred over the network to the PC Hub.
    * The PC Hub must automatically unpack the archive into the correct session directory, organized by device ID.
    * The final data files on the PC must be complete, uncorrupted, and identical to the files originally created on the
      Android device.

**Test Case 5: Fault Tolerance and Network Reconnection (FR8, NFR3)**

* **Objective:** To test the system's resilience to network interruptions, a common failure mode in distributed systems.
* **Procedure:**
    1. Start a synchronized recording session with at least one Android Spoke.
    2. During the recording, physically disconnect the Android device from the network (e.g., by turning off its Wi-Fi).
    3. Observe the PC GUI for at least 15 seconds.
    4. Reconnect the Android device to the network.
    5. Observe the PC GUI again.
    6. Stop the session.
* **Expected Outcome:**
    * The PC GUI must correctly update the device's status to "Offline" when the connection is lost.
    * The Android application **must not** stop recording; it should continue to save data locally.
    * Upon reconnection, the PC GUI must update the device's status back to "Online."
    * The reconnected device must correctly receive the final "Stop Session" command and successfully transfer its
      complete, uninterrupted data file to the PC.

By successfully passing these integration tests, the platform can be considered a validated, research-grade tool, ready
for use in pilot studies and real-world data collection scenarios.